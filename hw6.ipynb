{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 6: Clustering\n",
    "### Associated lectures: Lectures 15 and 16\n",
    "\n",
    "**Due date: Check the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:6}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 4. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Document clustering warm-up\n",
    "<hr>\n",
    "\n",
    "In this homework, we will explore a popular application of clustering called [**document clustering**](https://en.wikipedia.org/wiki/Document_clustering). A large amount of unlabeled text data is available out there (e.g., news, recipes, online Q&A, tweets), and clustering is a commonly used technique to organize this data in a meaningful way. \n",
    "\n",
    "As a warm up, in this exercise you will cluster sentences from a toy corpus. Later in the homework you will work with a real corpus. \n",
    "\n",
    "The code below extracts introductory sentences of Wikipedia articles on a set of queries. To run the code successfully, you will need the `wikipedia` package installed in the course environment. \n",
    "\n",
    "```\n",
    "conda activate cpsc330\n",
    "conda install -c conda-forge wikipedia\n",
    "```\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Run the code below which \n",
    "- extracts content of Wikipedia articles on a set of queries\n",
    "- tokenizes the text (i.e., separates sentences) and \n",
    "- stores the 2nd sentence in each article as a document representing that article\n",
    "\n",
    "> Feel free to experiment with Wikipedia queries of your choice. But stick to the provided list for the final submission so that it's easier for the TAs to grade your submission.\n",
    "\n",
    "> For tokenization we are using the `nltk` package. If you do not have this package in the course environment, you will have to install it.\n",
    "\n",
    "```\n",
    "conda activate cpsc330\n",
    "conda install -c anaconda nltk\n",
    "```\n",
    "\n",
    "Even if you have the package installed via the course `conda` environment, you might have to download `nltk` pre-trained models, which can be done with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yoykyl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yoykyl/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baguette food</td>\n",
       "      <td>It is distinguishable by its length and crisp crust.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana bread food</td>\n",
       "      <td>It is often a moist and sweet quick bread but some recipes are yeast raised.</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bread food</td>\n",
       "      <td>Throughout recorded history and around the world, it has been an important part of many cultures' diet.</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports analytics</td>\n",
       "      <td>The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film Moneyball.</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>football sport</td>\n",
       "      <td>Often competitive and organized, sports use, maintain, or improve physical ability and skills.</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>It belongs to a family of sports called hockey.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wiki query  \\\n",
       "0  baguette food       \n",
       "1  banana bread food   \n",
       "2  bread food          \n",
       "3  data science        \n",
       "4  sports analytics    \n",
       "5  football sport      \n",
       "6  ice hockey          \n",
       "\n",
       "                                                                                                                                                   text  \\\n",
       "0  It is distinguishable by its length and crisp crust.                                                                                                   \n",
       "1  It is often a moist and sweet quick bread but some recipes are yeast raised.                                                                           \n",
       "2  Throughout recorded history and around the world, it has been an important part of many cultures' diet.                                                \n",
       "3  Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).   \n",
       "4  The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film Moneyball.                             \n",
       "5  Often competitive and organized, sports use, maintain, or improve physical ability and skills.                                                         \n",
       "6  It belongs to a family of sports called hockey.                                                                                                        \n",
       "\n",
       "   n_words  \n",
       "0  10       \n",
       "1  16       \n",
       "2  20       \n",
       "3  24       \n",
       "4  21       \n",
       "5  17       \n",
       "6  10       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "queries = [\n",
    "    \"baguette food\",\n",
    "    \"banana bread food\",\n",
    "    \"bread food\",\n",
    "    \"data science\",\n",
    "    \"sports analytics\",\n",
    "    \"football sport\",\n",
    "    \"ice hockey\",\n",
    "]\n",
    "\n",
    "wiki_dict = {\"wiki query\": [], \"text\": [], \"n_words\": []}\n",
    "for i in range(len(queries)):\n",
    "    text = sent_tokenize(wikipedia.page(queries[i]).content)[1]\n",
    "    wiki_dict[\"text\"].append(text)\n",
    "    wiki_dict[\"n_words\"].append(len(word_tokenize(text)))\n",
    "    wiki_dict[\"wiki query\"].append(queries[i])\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_dict)\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toy corpus has six toy documents (`text` column in the dataframe) extracted from Wikipedia queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many clusters? \n",
    "rubric={points}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. If you are asked to cluster the documents from this toy corpus manually, how many clusters would you identify and how would you label each cluster?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would identify three clusters. The first label will be \"food\", the second one will be \"data analytics\", and the third one will be \"sports\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 `KMeans` with bag-of-words representation \n",
    "rubric={points}\n",
    "\n",
    "In the lecture, we saw that data representation plays a crucial role in clustering. Changing flattened representation of images to feature vectors extracted from pre-trained models greatly improved the quality of clustering. \n",
    "\n",
    "What kind of representation is suitable for text data? We have used bag-of-words representation to numerically encode text data before, where each document is represented with a vector of word frequencies. \n",
    "\n",
    "Let's try clustering documents with this simplistic representation.  \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create bag-of-words representation using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) with default arguments for the `text` column in `wiki_df` above.\n",
    "2. Cluster the encoded documents with [`KMeans` clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Use `random_state=42` (for reproducibility) and set `n_clusters` to the number you identified in the previous exercise.\n",
    "3. Store the clustering labels in `kmeans_bow_labels` variable below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2011</th>\n",
       "      <th>ability</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>analytics</th>\n",
       "      <th>and</th>\n",
       "      <th>application</th>\n",
       "      <th>are</th>\n",
       "      <th>around</th>\n",
       "      <th>been</th>\n",
       "      <th>...</th>\n",
       "      <th>technology</th>\n",
       "      <th>term</th>\n",
       "      <th>the</th>\n",
       "      <th>throughout</th>\n",
       "      <th>to</th>\n",
       "      <th>underlying</th>\n",
       "      <th>use</th>\n",
       "      <th>was</th>\n",
       "      <th>world</th>\n",
       "      <th>yeast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2011  ability  also  an  analytics  and  application  are  around  been  \\\n",
       "0  0     0        0     0   0          1    0            0    0       0      \n",
       "1  0     0        0     0   0          1    0            1    0       0      \n",
       "2  0     0        0     1   0          1    0            0    1       1      \n",
       "3  0     0        1     0   0          1    1            0    0       0      \n",
       "4  1     0        0     0   1          0    0            0    0       0      \n",
       "5  0     1        0     0   0          2    0            0    0       0      \n",
       "6  0     0        0     0   0          0    0            0    0       0      \n",
       "\n",
       "   ...  technology  term  the  throughout  to  underlying  use  was  world  \\\n",
       "0  ...  0           0     0    0           0   0           0    0    0       \n",
       "1  ...  0           0     0    0           0   0           0    0    0       \n",
       "2  ...  0           0     1    1           0   0           0    0    1       \n",
       "3  ...  1           0     1    0           0   1           0    0    0       \n",
       "4  ...  0           1     3    0           0   0           0    1    0       \n",
       "5  ...  0           0     0    0           0   0           1    0    0       \n",
       "6  ...  0           0     0    0           1   0           0    0    0       \n",
       "\n",
       "   yeast  \n",
       "0  0      \n",
       "1  1      \n",
       "2  0      \n",
       "3  0      \n",
       "4  0      \n",
       "5  0      \n",
       "6  0      \n",
       "\n",
       "[7 rows x 76 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(wiki_df['text'])\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "random_state = 42\n",
    "kmeans = KMeans(n_clusters=3, n_init='auto')\n",
    "kmeans.fit(bow_df)\n",
    "kmeans_bow_labels = kmeans.predict(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>n_words</th>\n",
       "      <th>bow_kmeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baguette food</td>\n",
       "      <td>It is distinguishable by its length and crisp crust.</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana bread food</td>\n",
       "      <td>It is often a moist and sweet quick bread but some recipes are yeast raised.</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bread food</td>\n",
       "      <td>Throughout recorded history and around the world, it has been an important part of many cultures' diet.</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports analytics</td>\n",
       "      <td>The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film Moneyball.</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>football sport</td>\n",
       "      <td>Often competitive and organized, sports use, maintain, or improve physical ability and skills.</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>It belongs to a family of sports called hockey.</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wiki query  \\\n",
       "0  baguette food       \n",
       "1  banana bread food   \n",
       "2  bread food          \n",
       "3  data science        \n",
       "4  sports analytics    \n",
       "5  football sport      \n",
       "6  ice hockey          \n",
       "\n",
       "                                                                                                                                                   text  \\\n",
       "0  It is distinguishable by its length and crisp crust.                                                                                                   \n",
       "1  It is often a moist and sweet quick bread but some recipes are yeast raised.                                                                           \n",
       "2  Throughout recorded history and around the world, it has been an important part of many cultures' diet.                                                \n",
       "3  Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).   \n",
       "4  The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film Moneyball.                             \n",
       "5  Often competitive and organized, sports use, maintain, or improve physical ability and skills.                                                         \n",
       "6  It belongs to a family of sports called hockey.                                                                                                        \n",
       "\n",
       "   n_words  bow_kmeans  \n",
       "0  10       2           \n",
       "1  16       2           \n",
       "2  20       2           \n",
       "3  24       2           \n",
       "4  21       1           \n",
       "5  17       0           \n",
       "6  10       2           "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df[\"bow_kmeans\"] = kmeans_bow_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Sentence embedding representation\n",
    "rubric={points}\n",
    "\n",
    "Bag-of-words representation is limited in that it does not take into account word ordering and context. There are other richer and more expressive representations of text which can be extracted using transfer learning. In this lab, we will use one such representation called sentence embedding representation, which uses deep learning models to generate dense, fixed-length vector representations for sentences. We will extract such representations using sentence transformer package. Sentence embedding takes into account context of words and semantic meaning of sentences and it is likely to work better when we are interested in clustering sentences based on their semantic similarity. \n",
    "\n",
    "```\n",
    "conda activate cpsc330\n",
    "conda install pytorch::pytorch torchvision torchaudio -c pytorch\n",
    "conda install -c conda-forge sentence-transformers\n",
    "```\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Run the code below to create sentence embedding representation of documents in our toy corpus. \n",
    "2. Cluster documents in our toy corpus encoded with this representation (`emb_sents`) and `KMeans` with following arguments: \n",
    "    - `random_state=42` (for reproducibility)\n",
    "    - `n_clusters`=the number of clusters you identified in 1.1\n",
    "3. Store the clustering labels in `kmeans_emb_labels` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cpsc330/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")\n",
    "\n",
    "# If this cell gives an error, try updating transformers with\n",
    "# pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186268</td>\n",
       "      <td>0.286701</td>\n",
       "      <td>0.058714</td>\n",
       "      <td>0.095216</td>\n",
       "      <td>-0.136911</td>\n",
       "      <td>0.091687</td>\n",
       "      <td>-0.029694</td>\n",
       "      <td>-0.104941</td>\n",
       "      <td>0.174888</td>\n",
       "      <td>0.482772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025579</td>\n",
       "      <td>-0.078822</td>\n",
       "      <td>0.470531</td>\n",
       "      <td>-0.209694</td>\n",
       "      <td>-0.379227</td>\n",
       "      <td>0.409230</td>\n",
       "      <td>-0.299176</td>\n",
       "      <td>0.155533</td>\n",
       "      <td>0.309285</td>\n",
       "      <td>0.124819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.202058</td>\n",
       "      <td>0.320748</td>\n",
       "      <td>0.190346</td>\n",
       "      <td>0.232201</td>\n",
       "      <td>0.600542</td>\n",
       "      <td>0.376622</td>\n",
       "      <td>-0.018205</td>\n",
       "      <td>-0.027148</td>\n",
       "      <td>-0.076680</td>\n",
       "      <td>0.381882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213062</td>\n",
       "      <td>-0.080009</td>\n",
       "      <td>0.422756</td>\n",
       "      <td>0.072724</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>-0.014596</td>\n",
       "      <td>-0.202858</td>\n",
       "      <td>-0.162947</td>\n",
       "      <td>-0.190418</td>\n",
       "      <td>-0.002013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.022417</td>\n",
       "      <td>0.217159</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.240857</td>\n",
       "      <td>0.358046</td>\n",
       "      <td>-0.053310</td>\n",
       "      <td>-0.328075</td>\n",
       "      <td>0.190012</td>\n",
       "      <td>0.244470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265412</td>\n",
       "      <td>-0.415595</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.052484</td>\n",
       "      <td>0.345947</td>\n",
       "      <td>0.110091</td>\n",
       "      <td>0.405441</td>\n",
       "      <td>0.197792</td>\n",
       "      <td>-0.058253</td>\n",
       "      <td>0.212376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270839</td>\n",
       "      <td>0.187923</td>\n",
       "      <td>-0.010778</td>\n",
       "      <td>0.318688</td>\n",
       "      <td>-0.092012</td>\n",
       "      <td>-0.204806</td>\n",
       "      <td>0.038101</td>\n",
       "      <td>-0.117648</td>\n",
       "      <td>0.084057</td>\n",
       "      <td>-0.170613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069944</td>\n",
       "      <td>-0.222987</td>\n",
       "      <td>0.092472</td>\n",
       "      <td>0.309444</td>\n",
       "      <td>0.289248</td>\n",
       "      <td>0.021022</td>\n",
       "      <td>-0.032694</td>\n",
       "      <td>-0.624041</td>\n",
       "      <td>0.358924</td>\n",
       "      <td>-0.034092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101732</td>\n",
       "      <td>-0.185133</td>\n",
       "      <td>-0.107289</td>\n",
       "      <td>0.148191</td>\n",
       "      <td>0.458463</td>\n",
       "      <td>-0.555427</td>\n",
       "      <td>-0.073999</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>-0.172459</td>\n",
       "      <td>-0.072560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428589</td>\n",
       "      <td>-0.340838</td>\n",
       "      <td>-0.159756</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.316263</td>\n",
       "      <td>0.090864</td>\n",
       "      <td>-0.162352</td>\n",
       "      <td>-0.225394</td>\n",
       "      <td>0.417016</td>\n",
       "      <td>-0.179009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.124750</td>\n",
       "      <td>0.327760</td>\n",
       "      <td>0.287335</td>\n",
       "      <td>-0.108246</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>-0.123068</td>\n",
       "      <td>0.256753</td>\n",
       "      <td>-0.186897</td>\n",
       "      <td>0.207025</td>\n",
       "      <td>0.225442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476354</td>\n",
       "      <td>-0.425226</td>\n",
       "      <td>0.153235</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.104009</td>\n",
       "      <td>-0.137497</td>\n",
       "      <td>0.265884</td>\n",
       "      <td>0.431444</td>\n",
       "      <td>0.173652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.108902</td>\n",
       "      <td>0.087364</td>\n",
       "      <td>0.119847</td>\n",
       "      <td>-0.050133</td>\n",
       "      <td>0.240210</td>\n",
       "      <td>-0.082101</td>\n",
       "      <td>0.190988</td>\n",
       "      <td>0.436203</td>\n",
       "      <td>-0.085558</td>\n",
       "      <td>-0.012788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145414</td>\n",
       "      <td>0.160032</td>\n",
       "      <td>0.071931</td>\n",
       "      <td>0.147640</td>\n",
       "      <td>0.043789</td>\n",
       "      <td>0.253031</td>\n",
       "      <td>-0.009357</td>\n",
       "      <td>0.415206</td>\n",
       "      <td>0.142142</td>\n",
       "      <td>0.130365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.186268  0.286701  0.058714  0.095216 -0.136911  0.091687 -0.029694   \n",
       "1 -0.202058  0.320748  0.190346  0.232201  0.600542  0.376622 -0.018205   \n",
       "2 -0.022417  0.217159  0.022694  0.003616  0.240857  0.358046 -0.053310   \n",
       "3  0.270839  0.187923 -0.010778  0.318688 -0.092012 -0.204806  0.038101   \n",
       "4 -0.101732 -0.185133 -0.107289  0.148191  0.458463 -0.555427 -0.073999   \n",
       "5  0.124750  0.327760  0.287335 -0.108246  0.024023 -0.123068  0.256753   \n",
       "6  0.108902  0.087364  0.119847 -0.050133  0.240210 -0.082101  0.190988   \n",
       "\n",
       "          7         8         9  ...       758       759       760       761  \\\n",
       "0 -0.104941  0.174888  0.482772  ...  0.025579 -0.078822  0.470531 -0.209694   \n",
       "1 -0.027148 -0.076680  0.381882  ...  0.213062 -0.080009  0.422756  0.072724   \n",
       "2 -0.328075  0.190012  0.244470  ...  0.265412 -0.415595  0.003036  0.052484   \n",
       "3 -0.117648  0.084057 -0.170613  ...  0.069944 -0.222987  0.092472  0.309444   \n",
       "4  0.001754 -0.172459 -0.072560  ...  0.428589 -0.340838 -0.159756  0.388600   \n",
       "5 -0.186897  0.207025  0.225442  ...  0.476354 -0.425226  0.153235  0.176923   \n",
       "6  0.436203 -0.085558 -0.012788  ...  0.145414  0.160032  0.071931  0.147640   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0 -0.379227  0.409230 -0.299176  0.155533  0.309285  0.124819  \n",
       "1  0.016025 -0.014596 -0.202858 -0.162947 -0.190418 -0.002013  \n",
       "2  0.345947  0.110091  0.405441  0.197792 -0.058253  0.212376  \n",
       "3  0.289248  0.021022 -0.032694 -0.624041  0.358924 -0.034092  \n",
       "4  0.316263  0.090864 -0.162352 -0.225394  0.417016 -0.179009  \n",
       "5  0.036420  0.104009 -0.137497  0.265884  0.431444  0.173652  \n",
       "6  0.043789  0.253031 -0.009357  0.415206  0.142142  0.130365  \n",
       "\n",
       "[7 rows x 768 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_sents = embedder.encode(wiki_df[\"text\"])\n",
    "emb_sent_df = pd.DataFrame(emb_sents, index=wiki_df.index)\n",
    "emb_sent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans_embedding = KMeans(n_clusters = 3, n_init = 'auto')\n",
    "kmeans_embedding.fit(emb_sent_df)\n",
    "kmeans_emb_labels = kmeans_embedding.predict(emb_sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>n_words</th>\n",
       "      <th>bow_kmeans</th>\n",
       "      <th>emb_kmeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baguette food</td>\n",
       "      <td>It is distinguishable by its length and crisp crust.</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana bread food</td>\n",
       "      <td>It is often a moist and sweet quick bread but some recipes are yeast raised.</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bread food</td>\n",
       "      <td>Throughout recorded history and around the world, it has been an important part of many cultures' diet.</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports analytics</td>\n",
       "      <td>The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film Moneyball.</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>football sport</td>\n",
       "      <td>Often competitive and organized, sports use, maintain, or improve physical ability and skills.</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>It belongs to a family of sports called hockey.</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wiki query  \\\n",
       "0  baguette food       \n",
       "1  banana bread food   \n",
       "2  bread food          \n",
       "3  data science        \n",
       "4  sports analytics    \n",
       "5  football sport      \n",
       "6  ice hockey          \n",
       "\n",
       "                                                                                                                                                   text  \\\n",
       "0  It is distinguishable by its length and crisp crust.                                                                                                   \n",
       "1  It is often a moist and sweet quick bread but some recipes are yeast raised.                                                                           \n",
       "2  Throughout recorded history and around the world, it has been an important part of many cultures' diet.                                                \n",
       "3  Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).   \n",
       "4  The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film Moneyball.                             \n",
       "5  Often competitive and organized, sports use, maintain, or improve physical ability and skills.                                                         \n",
       "6  It belongs to a family of sports called hockey.                                                                                                        \n",
       "\n",
       "   n_words  bow_kmeans  emb_kmeans  \n",
       "0  10       2           1           \n",
       "1  16       2           1           \n",
       "2  20       2           2           \n",
       "3  24       2           0           \n",
       "4  21       1           0           \n",
       "5  17       0           0           \n",
       "6  10       2           0           "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df[\"emb_kmeans\"] = kmeans_emb_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 DBSCAN with cosine distance  \n",
    "rubric={points}\n",
    "\n",
    "Now try [`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on our toy dataset. K-Means is kind of bound to the Euclidean distance because it is based on the notion of means. With `DBSCAN` we can try different distance metrics. In the context of text data, [cosine similarities](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) or cosine distances tend to work well. Given vectors $u$ and $v$, the **cosine distance** between the vectors is defined as: \n",
    "\n",
    "$$distance_{cosine}(u,v) = 1 - (\\frac{u \\cdot v}{\\left\\lVert u\\right\\rVert_2 \\left\\lVert v\\right\\rVert_2})$$\n",
    "\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Cluster documents in our toy corpus encoded with sentence embedding representation (`emb_sents`) and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan#sklearn.cluster.DBSCAN) with `metric='cosine'`. You will have to set appropriate values for the hyperparamters `eps` and `min_samples` to get meaningful clusters, as default values of these hyperparameters are unlikely to work well on this toy dataset.\n",
    "2. Store the clustering labels in the `dbscan_emb_labels` variable below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster memberships:[-1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(emb_sents)\n",
    "print(\"Cluster memberships:{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib\n",
    "results = {}\n",
    "\n",
    "for eps in np.arange(1, 4, 0.5):\n",
    "    for min_samples in range(1, 7):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\")\n",
    "        clusters = dbscan.fit_predict(emb_sents)\n",
    "        results[(eps, min_samples)] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_ignore"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbscan_emb_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"emb_dbscan\"] = dbscan_emb_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Hierarchical clustering with sentence embedding representation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Try hierarchical clustering on `emb_sents`. In particular\n",
    "1. Create and show a dendrogram with `complete` linkage and `metric='cosine'` on this toy dataset.\n",
    "2. Create flat clusters using `fcluster` with appropriate hyperparameters and store cluster labels to `hier_emb_labels` variable below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hier_emb_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hier_emb_labels = fcluster(Z, 3, criterion=\"maxclust\") # alternative solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"emb_hierarchical\"] = hier_emb_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.6 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Reflect on and discuss the clustering results of the methods you explored in the previous exercises, focusing on the following points:    \n",
    "    - effect of input representation on clustering results\n",
    "    - whether the clustering results match with your intuitions and the challenges associated with getting the desired clustering results with each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.7 Visualizing clusters\n",
    "rubric={points:4}\n",
    "\n",
    "\n",
    "One approach to working with unlabeled data is visualization. That said, our data is high-dimensional, making it challenging to visualize. Take sentence embedding representation as an example: each instance is depicted in 768 dimensions. To visualize such high-dimensional data, we can employ dimensionality reduction techniques to extract the most significant 2 or 3 components, and then visualize this low-dimensional data.\n",
    "\n",
    "Given data as a `numpy` array and corresponding cluster assignments, the `plot_umap_clusters` function below transforms the data by applying dimensionality reduction technique called [UMAP](https://umap-learn.readthedocs.io/en/latest/) to it and plots the transformed data with different colours for different clusters. \n",
    "\n",
    "> *Note: At this point we are using this function only for visualization and you are not expected to understand the UMAP part.* \n",
    "\n",
    "You'll have to install the `umap-learn` package in the course conda environment either with `conda` or `pip`, as described in the [documentation](https://umap-learn.readthedocs.io/en/latest/index.html). \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge umap-learn\n",
    "```\n",
    "or\n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> pip install umap-learn \n",
    "```\n",
    "\n",
    "If you get an error with the import below try\n",
    "\n",
    "```\n",
    "pip install --upgrade numba umap-learn\n",
    "```\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Visualize the clusters created by the methods above using `plot_umap_clusters` function below. In other words, visualize clusters identified by each of the methods below. \n",
    "    - K-Means with bag-of-words representation \n",
    "    - K-Means with sentence embedding representation\n",
    "    - DBSCAN with sentence embedding representation \n",
    "    - Flat cluster of hierarchical clustering with sentence embedding representation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_umap_clusters(\n",
    "    data,\n",
    "    cluster_labels,\n",
    "    raw_sents=wiki_df[\"text\"],\n",
    "    show_labels=False,\n",
    "    size=50,\n",
    "    n_neighbors=15,\n",
    "    title=\"UMAP visualization\",\n",
    "    ignore_noise=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Carry out dimensionality reduction using UMAP and plot 2-dimensional clusters.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data : numpy array\n",
    "        data as a numpy array\n",
    "    cluster_labels : list\n",
    "        cluster labels for each row in the dataset\n",
    "    raw_sents : list\n",
    "        the original raw sentences for labeling datapoints\n",
    "    show_labels : boolean\n",
    "        whether you want to show labels for points or not (default: False)\n",
    "    size : int\n",
    "        size of points in the scatterplot\n",
    "    n_neighbors : int\n",
    "        n_neighbors hyperparameter of UMAP. See the documentation.\n",
    "    title : str\n",
    "        title for the visualization plot\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    None. Shows the clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=42)\n",
    "    Z = reducer.fit_transform(data)  # reduce dimensionality\n",
    "    umap_df = pd.DataFrame(data=Z, columns=[\"dim1\", \"dim2\"])\n",
    "    umap_df[\"cluster\"] = cluster_labels\n",
    "\n",
    "    if ignore_noise:\n",
    "        umap_df = umap_df[umap_df[\"cluster\"] != -1]\n",
    "\n",
    "    labels = np.unique(umap_df[\"cluster\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        umap_df[\"dim1\"],\n",
    "        umap_df[\"dim2\"],\n",
    "        c=umap_df[\"cluster\"],\n",
    "        cmap=\"tab20b\",\n",
    "        s=size,\n",
    "        #edgecolors=\"k\",\n",
    "        #linewidths=0.1,\n",
    "    )\n",
    "\n",
    "    legend = ax.legend(*scatter.legend_elements(), loc=\"best\", title=\"Clusters\")\n",
    "    ax.add_artist(legend)\n",
    "\n",
    "    if show_labels:\n",
    "        x = umap_df[\"dim1\"].tolist()\n",
    "        y = umap_df[\"dim2\"].tolist()\n",
    "        for i, txt in enumerate(raw_sents):\n",
    "            ax.annotate(\" \".join(txt.split()[:10]), (x[i], y[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.7\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: [Food.com](https://www.food.com/) recipes \n",
    "<hr>\n",
    "\n",
    "Now that we have applied document clustering on a toy corpus, let's move to a more realistic corpus. \n",
    "\n",
    "In the lecture, we worked on an activity of manually clustering food items and discussed challenges associated with it. We also applied different clustering algorithms to cluster food images. We'll continue this theme of clustering food items in this lab. But instead of images we will cluster textual description of food items, i.e., recipe names.   \n",
    "\n",
    "In this lab, we will work with a sample of [Kaggle's Food.com recipes corpus](https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions). This corpus contains 180K+ recipes and 700K+ recipe reviews. In this lab, we'll only focus on recipes and **not** on reviews. The recipes are present in `RAW_recipes.csv`. Our goal is to find categories or groupings of recipes from this corpus based on their names. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Download [`RAW_recipes.csv`](https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions?select=RAW_recipes.csv) and put it under the `data` directory in the homework folder. \n",
    "- Run the code below. The dataset is quite large, and in this assignment, for speed, you will work with a sample of the dataset. The function `get_recipes_sample` below carries out some preliminary preprocessing and returns a sample of the recipes with most frequent tags. \n",
    "\n",
    "> *Note: Depending upon the capacity of your computer, feel free to increase or decrease the size of this sample by changing the value for `n_tags`. If you decide to go with a different value of `n_tags`, state it clearly in Exercise 2.1 so that the grader knows about it.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_recipes_df = pd.read_csv(\"data/RAW_recipes.csv\")\n",
    "orig_recipes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipes_sample(orig_recipes_df, n_tags=300, min_len=5):\n",
    "    orig_recipes_df = orig_recipes_df.dropna()  # Remove rows with NaNs.\n",
    "    orig_recipes_df = orig_recipes_df.drop_duplicates(\n",
    "        \"name\"\n",
    "    )  # Remove rows with duplicate names.\n",
    "    # Remove rows where recipe names are too short (< 5 characters).\n",
    "    orig_recipes_df = orig_recipes_df[orig_recipes_df[\"name\"].apply(len) >= min_len]\n",
    "    # Only consider the rows where tags are one of the most frequent n tags.\n",
    "    first_n = orig_recipes_df[\"tags\"].value_counts()[0:n_tags].index.tolist()\n",
    "    recipes_df = orig_recipes_df[orig_recipes_df[\"tags\"].isin(first_n)]\n",
    "    return recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df = get_recipes_sample(orig_recipes_df)\n",
    "recipes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the rest of the homework, we will use `recipes_df` above, which is a subset of the original dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Longest and shorter recipe names \n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Print the shortest and longest recipe names (length in terms of number of characters) from `recipes_df`. If there is more than one recipe with the same shortest/longest length, store **one** of them in `shortest_recipe` and/or `longest_recipe` as a **string**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortest_recipe = None\n",
    "longest_recipe = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 More EDA\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create a word cloud for the recipe names. You can use [the `wordcloud` package](https://github.com/amueller/word_cloud) for this, which you will have to install in the course environment.\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Representing recipe names\n",
    "rubric={points:3}\n",
    "\n",
    "The next step is creating a representation of recipe names. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Similar to Exercise 1, create sentence embedding representation of recipe names (`name` column in `recipes_df`).  For the rest of the homework, we'll stick to the sentence embedding representation of recipe names.\n",
    "\n",
    "\n",
    "> You might have to convert the recipe names to a list (`recipes_df[\"name\"].tolist()`) for the embedder to work\n",
    "> *If you create a dataframe with sentence embedding representation, set the index to `recipes_df.index` so that the indices match with the indices of the sample we are working with.*  \n",
    "> **This might take a while to run.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Clustering recipe names\n",
    "<hr>\n",
    "\n",
    "In this exercise you'll cluster recipe names with some of the clustering algorithms we have seen in class. This will also involve making some attempts to pick reasonable hyperparameter values for each clustering method based on the quality of the resulting clusters. For example, for KMeans, you need to specify the number of clusters in advance, which is often challenging on real-world datasets. For DBSCAN, you need to pick appropriate `eps` and `min_samples`. For hierarchical clustering, you need to pick a suitable linkage criterion, distance metric, and prune the tree so that it's possible to visualize and interpret it. \n",
    "\n",
    "Here are some methods which may help you with picking reasonable values for the hyperparameters. \n",
    "- Visualize the Elbow plot (KMeans). \n",
    "- Visualize Silhouette plots. \n",
    "- Visualize resulting clusters using `plot_umap_clusters` function from Exercise 1. \n",
    "- Sample some recipes from each cluster, manually inspect whether there are coherent semantic themes. (For this, you may use the function `print_clusters` given below.) \n",
    "        \n",
    "> You may use the [`yellowbrick`](https://www.scikit-yb.org/en/latest/) package for visualizing the Elbow plot and the Silhouette plots. You can intall it with\n",
    "\n",
    "```conda install -c districtdatalabs yellowbrick```\n",
    "\n",
    "**Note that the process of picking reasonable hyperparameter values will be exploratory, iterative, and will involve manual inspection and judgment, as there is no ground truth to verify how well the model is doing. In your solutions, please do not include everything you try. Only present the results of the most informative trials. Add a narrative to your answer so that it's easy for the grader to follow your choices and reasoning.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(recipes_df, cluster_labels, n_recipes=10, replace=False, random_state=None):\n",
    "    \"\"\"\n",
    "    Given recipes_df containing recipe names and cluster assignment (labels), \n",
    "    sample and print n_recipes recipes per cluster. \n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    recipe_df : pandas dataframe \n",
    "        recipes dataframe containing recipe names in the \"name\" column\n",
    "    cluster_labels : ndarray or a list\n",
    "        cluster labels for each row in recipes_df \n",
    "    n_recipes : int\n",
    "        number of examples to sample from each cluster\n",
    "    replace: bool\n",
    "        replace flag to pass to the sampling of recipe names\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    None\n",
    "    \"\"\"    \n",
    "    \n",
    "    grouped = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"name\": recipes_df[\"name\"],\n",
    "                \"cluster_label\": cluster_labels,\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"cluster_label\")\n",
    "        .groupby(\"cluster_label\")    \n",
    "    )\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        print(f\"Cluster {name}\")        \n",
    "        print((\"----------\").format(\"\"))        \n",
    "        print(\"\\n\".join(group.sample(n_recipes, random_state=random_state)['name'].tolist()))\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.1 K-Means\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Cluster recipe titles using KMeans. Make some attempts to determine the optimal number of clusters. \n",
    "2. Pick one or two best models and justify your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": [
     "otter_ignore"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2 DBSCAN\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Cluster recipe names using `DBSCAN` with `metric=\"cosine\"`. Make some attempts to tune the  hyperparameters `eps` and `min_samples`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Hierarchical clustering\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try hierarchical clustering with `metric=\"cosine\"` on this problem. Show a dendrogram by using a suitable truncation method. \n",
    "2. Create flat clusters by cutting the tree at the appropriate level. \n",
    "\n",
    "> *Note: Try orientation=\"left\" of `dendrogram` for better readability of the dendrogram.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.4 Manual interpretation of clusters\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Label the topics/themes you see in the clusters created by different clustering methods.  \n",
    "2. Do you see a common theme across clusters created by different clustering methods? Do you see any differences between the clusters created by different clustering methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
